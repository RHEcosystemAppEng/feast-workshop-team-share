{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9dc1a8-f4a2-484b-8c7a-0fb5a832a481",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib/python3.9/site-packages (24.0)\n",
      "Requirement already satisfied: pyarrow in /opt/app-root/lib/python3.9/site-packages (15.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /opt/app-root/lib/python3.9/site-packages (from pyarrow) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -q -r ../feast_modelregistry/requirements.txt\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2375f-341c-4f4a-a214-f7b352c1dfa6",
   "metadata": {},
   "source": [
    "# Feast Remote Offline Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64174e-0c37-4617-a1de-4c6bcb9713e3",
   "metadata": {},
   "source": [
    "Defines an `RemoteOfflineStore` implementation of the `OfflineStore` interface delegating the actual data manipulation to\n",
    "the remote Offline Store realized by the Arrow Flight server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7babf-be89-4dcb-a76c-c781f39c8f65",
   "metadata": {},
   "source": [
    "## Constants and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62878e25-942f-4762-b493-61ad38b8b6d9",
   "metadata": {},
   "source": [
    "**Note**: This step is needed only because of the nature of the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee7ade9-9405-4ff3-94cb-aad8eefdc36b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "psqlHost = 'postgresql.feast.svc.cluster.local'\n",
    "psqlPort = 5432\n",
    "psqlUsername = 'feast'\n",
    "psqlPassword = 'feast'\n",
    "psqlDb = 'feast'\n",
    "psqlSchema = 'feast'\n",
    "\n",
    "mnistTableName = 'mnist_source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3db197-d9f0-42e4-868a-c1d26c27d739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import uuid\n",
    "\n",
    "from feast.feature_view import FeatureView\n",
    "from feast.infra.offline_stores.offline_store import (\n",
    "    OfflineStore,\n",
    "    RetrievalJob,\n",
    ")\n",
    "from feast.infra.registry.registry import Registry\n",
    "from feast.on_demand_feature_view import OnDemandFeatureView\n",
    "from feast.repo_config import RepoConfig\n",
    "from feast.usage import log_exceptions_and_usage\n",
    "import pyarrow.flight as fl\n",
    "from sqlalchemy import create_engine, MetaData, Table, select\n",
    "from typing import List, Union, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99729252-6d97-4b0e-b4b0-9f82a4e5d9cf",
   "metadata": {},
   "source": [
    "## Prepare the input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4d227-2422-4304-9600-b8d57f9adf44",
   "metadata": {},
   "source": [
    "Connect the MNIST data source to read the list of (`image_id`, `event_timestamp`) pairs used to fetch the historical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c181ea3b-b7cd-49ec-847f-bf028ba1bcfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27091/2869619936.py:3: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  table = Table(mnistTableName, metadata, autoload=True, autoload_with=engine)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2016-05-11 16:14:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-11 17:14:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-05-11 18:14:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-11 19:14:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>2016-05-11 20:14:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id     event_timestamp\n",
       "0         5 2016-05-11 16:14:44\n",
       "1         0 2016-05-11 17:14:44\n",
       "2         4 2016-05-11 18:14:44\n",
       "3         1 2016-05-11 19:14:44\n",
       "4         9 2016-05-11 20:14:44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine(f'postgresql+psycopg2://{psqlUsername}:{psqlPassword}@{psqlHost}:{str(psqlPort)}/{psqlDb}')\n",
    "metadata = MetaData()\n",
    "table = Table(mnistTableName, metadata, autoload=True, autoload_with=engine)\n",
    "\n",
    "columns = [table.c.image_id, table.c.ts.label('event_timestamp')]\n",
    "stmt = select(columns)\n",
    "\n",
    "image_ids = []\n",
    "ts = []\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(stmt)\n",
    "    for row in result:\n",
    "        image_ids.append(row['image_id'])\n",
    "        ts.append(row['event_timestamp'])\n",
    "\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"image_id\": image_ids,\n",
    "        \"event_timestamp\": ts,\n",
    "    }\n",
    ")\n",
    "entity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10269a-5249-4dc6-b57e-89ec9d4b5261",
   "metadata": {},
   "source": [
    "## Define the RemoteOfflineStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949df0ba-7c50-4474-9886-fd3a091036f3",
   "metadata": {},
   "source": [
    "Basic implementation of the `RemoteOfflineStore` and `RemoteRetrievalJob` classes following the data exchange protocol\n",
    "defined in the [README.md](./README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6193ee51-91e0-4017-bbe6-4b29360c29a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RemoteRetrievalJob(RetrievalJob):\n",
    "    def __init__(\n",
    "        self,\n",
    "        arrow_host,\n",
    "        arrow_port,\n",
    "        feature_refs: List[str],\n",
    "        entity_df: Union[pd.DataFrame, str],\n",
    "        # TODO add missing parameters from the OfflineStore API\n",
    "    ):\n",
    "        # Generate unique command identifier\n",
    "        self.command = str(uuid.uuid4())\n",
    "        # Initialize the client connection\n",
    "        self.client = pa.flight.connect(f\"grpc://{arrow_host}:{arrow_port}\")\n",
    "        # Put API parameters\n",
    "        self._put_parameters(feature_refs, entity_df)\n",
    "\n",
    "    def _put_parameters(self, feature_refs, entity_df):\n",
    "        record_batch_entity = pa.Table.from_pandas(entity_df)\n",
    "        flight_info_entity = pa.flight.FlightDescriptor.for_command(self.command)\n",
    "        writer, _ = self.client.do_put(flight_info_entity,\n",
    "                                       record_batch_entity.schema.with_metadata({\n",
    "                                           'command': self.command, \n",
    "                                           'api': 'get_historical_features', \n",
    "                                           'param': 'entity_df'}))\n",
    "        writer.write_table(record_batch_entity)\n",
    "        writer.close()\n",
    "\n",
    "        features_array = pa.array(feature_refs)\n",
    "        features_batch = pa.RecordBatch.from_arrays([features_array], ['features'])\n",
    "        writer, _ = self.client.do_put(flight_info_entity,\n",
    "                                       features_batch.schema.with_metadata({\n",
    "                                           'command': self.command, \n",
    "                                           'api': 'get_historical_features', \n",
    "                                           'param': 'features'}))\n",
    "        writer.write_batch(features_batch)\n",
    "        writer.close()\n",
    "\n",
    "    # Invoked to realize the Pandas DataFrame\n",
    "    def _to_df_internal(self, timeout: Optional[int] = None) -> pd.DataFrame:\n",
    "        # We use arrow format because it gives better control of the table schema\n",
    "        return self._to_arrow_internal().to_pandas()\n",
    "\n",
    "    # Invoked to synchronously execute the underlying query and return the result as an arrow table\n",
    "    # This is where do_get service is invoked\n",
    "    def _to_arrow_internal(self, timeout: Optional[int] = None) -> pa.Table:\n",
    "        upload_descriptor = pa.flight.FlightDescriptor.for_command(self.command)\n",
    "        flight = self.client.get_flight_info(upload_descriptor)\n",
    "        ticket = flight.endpoints[0].ticket\n",
    "\n",
    "        reader = self.client.do_get(ticket)\n",
    "        return reader.read_all()\n",
    "\n",
    "    @property\n",
    "    def on_demand_feature_views(self) -> List[OnDemandFeatureView]:\n",
    "        return []\n",
    "\n",
    "\n",
    "class RemoteOfflineStore(OfflineStore):\n",
    "    def __init__(\n",
    "        self,\n",
    "        arrow_host,\n",
    "        arrow_port\n",
    "    ):\n",
    "        self.arrow_host = arrow_host\n",
    "        self.arrow_port = arrow_port\n",
    "\n",
    "    @log_exceptions_and_usage(offline_store=\"remote\")\n",
    "    def get_historical_features(\n",
    "        self,\n",
    "        config: RepoConfig,\n",
    "        feature_views: List[FeatureView],\n",
    "        feature_refs: List[str],\n",
    "        entity_df: Union[pd.DataFrame, str],\n",
    "        registry: Registry = None,\n",
    "        project: str = '',\n",
    "        full_feature_names: bool = False,\n",
    "    ) -> RemoteRetrievalJob:\n",
    "        return RemoteRetrievalJob(self.arrow_host, self.arrow_port, feature_refs, entity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d073921-e577-4914-915f-10ff2c3788dc",
   "metadata": {},
   "source": [
    "## Testing the get_historical_features API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b577e169-2dcb-48cd-b42d-82d28b2231a3",
   "metadata": {},
   "source": [
    "Create the `RemoteOfflineStore` instance to interact with the Arrow Flight server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39b7f54-53e0-445f-93b9-5387db619086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arrow_host=\"0.0.0.0\"\n",
    "arrow_port=8815\n",
    "offlineStore = RemoteOfflineStore(arrow_host=arrow_host, arrow_port=arrow_port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10914a69-a717-4711-a22a-90b1bd781228",
   "metadata": {
    "tags": []
   },
   "source": [
    "Verify there are no pending flights before we begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dbd42ed-f51f-498d-9c83-8105cb50e169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_client = pa.flight.connect(f\"grpc://{arrow_host}:{arrow_port}\")\n",
    "flights = test_client.list_flights()\n",
    "size = len(list(flights))\n",
    "assert size == 0, f\"Found {size} existing flights, instead of none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80515d-d5bc-4be0-ba8c-7ff6b1c45621",
   "metadata": {},
   "source": [
    "Invoke the `get_historical_features` API on the `OfflineStore` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a576a1ef-d125-49db-9493-991b0ee17940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 0 to 5000\n",
      "Fetching rows from 5000 to 10000\n",
      "Fetching rows from 10000 to 15000\n",
      "Fetching rows from 15000 to 20000\n",
      "Fetching rows from 20000 to 25000\n",
      "Fetching rows from 25000 to 30000\n",
      "Fetching rows from 30000 to 35000\n",
      "Fetching rows from 35000 to 40000\n",
      "Fetching rows from 40000 to 45000\n",
      "Fetching rows from 45000 to 50000\n",
      "Fetching rows from 50000 to 55000\n",
      "Fetching rows from 55000 to 60000\n",
      "Fetching rows from 60000 to 65000\n",
      "Fetching rows from 65000 to 70000\n",
      "CPU times: user 1.4 s, sys: 362 ms, total: 1.77 s\n",
      "Wall time: 18.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-11 21:14:44</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.4117647058823529,0.9882...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.9058823529411765,0.9882...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.8117647058823529,0.9882...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.050980392156862744,0.36...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-12 00:14:44</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-12 06:14:44</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>2016-05-12 07:14:44</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.050...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.160...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1568627...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6470588...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16862745098...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.53725490196...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26274509803...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2016-05-12 10:14:44</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2627450...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0980392...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.419...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id     event_timestamp  \\\n",
       "0         2 2016-05-11 21:14:44   \n",
       "1         1 2016-05-12 00:14:44   \n",
       "2         1 2016-05-12 06:14:44   \n",
       "3         7 2016-05-12 07:14:44   \n",
       "4         6 2016-05-12 10:14:44   \n",
       "\n",
       "                                           feature_1  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                           feature_2  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                           feature_3  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                           feature_4  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                           feature_5  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                           feature_6  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                           feature_7  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                           feature_8  ...  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...  ...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...  ...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...  ...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...  ...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...  ...   \n",
       "\n",
       "                                          feature_20  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.4117647058823529,0.9882...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2627450...   \n",
       "\n",
       "                                          feature_21  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.9058823529411765,0.9882...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.050...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0980392...   \n",
       "\n",
       "                                          feature_22  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.8117647058823529,0.9882...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.160...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.419...   \n",
       "\n",
       "                                          feature_23  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.050980392156862744,0.36...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1568627...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                          feature_24  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6470588...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                          feature_25  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16862745098...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                          feature_26  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.53725490196...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                          feature_27  \\\n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26274509803...   \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...   \n",
       "\n",
       "                                          feature_28 number  \n",
       "0  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...      2  \n",
       "1  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...      1  \n",
       "2  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...      1  \n",
       "3  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...      7  \n",
       "4  (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...      6  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "# Create the features list\n",
    "historical_df = pd.DataFrame()\n",
    "features = [f\"mnist:feature_{i+1}\" for i in range(28)]\n",
    "features.append(\"mnist:number\")\n",
    "\n",
    "# Fetch in chuncks of 5000 records\n",
    "batch_size = 5000\n",
    "offset = 0\n",
    "while offset < len(entity_df):\n",
    "    end_index = min(len(entity_df), offset + batch_size)\n",
    "    print(f\"Fetching rows from {offset} to {end_index}\")\n",
    "    batch_entity_df = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"image_id\": entity_df['image_id'][offset: end_index],\n",
    "            \"event_timestamp\": entity_df['event_timestamp'][offset: end_index],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    offset += batch_size\n",
    "    # TODO adjust feature_refs and fetch feature_views from repo\n",
    "    retrievalJob = offlineStore.get_historical_features(\n",
    "        config=None,\n",
    "        feature_views=[],\n",
    "        entity_df=batch_entity_df,\n",
    "        feature_refs=features,\n",
    "    )\n",
    "    \n",
    "    training_df = retrievalJob.to_df()\n",
    "    historical_df = pd.concat([historical_df, training_df], ignore_index=True)\n",
    "    # break\n",
    "\n",
    "historical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "271c216f-7920-400e-b691-d7f9ebbb1799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(historical_df) == 70000, f\"Found {len(historical_df)} instead of 70000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d7be9-70e6-4565-9fcc-6af4b1885b43",
   "metadata": {},
   "source": [
    "Verify there are no pending flights after the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d711f7f-1f7e-4051-b5d7-815cbaedf37c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flights = test_client.list_flights()\n",
    "size = len(list(flights))\n",
    "assert size == 0, f\"Found {size} existing flights, instead of none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c0d2f-bd8b-414f-b33e-2e334d0d9c3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Additional validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a199265c-9d75-480e-a85a-4046bfb2aec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actions = test_client.list_actions()\n",
    "assert len(actions) == 0, f\"Found {len(actions)} existing flights, instead of none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22abf0-607d-415b-b8f7-43709bffafe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
